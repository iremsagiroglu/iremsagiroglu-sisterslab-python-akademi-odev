{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all_data Projesi\n",
    "\n",
    "1. Bütün text küçük harfe çevrilecek\n",
    "\n",
    "2. Türkçe karakterler replace edilecek\n",
    "\n",
    "ş -> s\n",
    "\n",
    "ı -> i\n",
    "\n",
    "ö -> o\n",
    "\n",
    "ğ -> g\n",
    "\n",
    "ç -> c\n",
    "\n",
    "ü -> u\n",
    "\n",
    "3. Noktalama işaretleri kaldırılacak \n",
    "\n",
    "4. Bir satır tamamen sayılardan oluşuyorsa satırı yeni dosyaya ekleme \n",
    "\n",
    "5. Her satırda bir cümle olacak şekilde satırları düzenleyin \n",
    "\n",
    "6. Duplicate veri barındırmayacak\n",
    "\n",
    "+++\n",
    "\n",
    "7. Her satırda tek cümle olacak\n",
    "\n",
    "8. Text'te yer alan en çok kullanılan 100 kelime yeni bir dosyaya aktaracak ve aşağıdaki şekilde aynı hizada olacak şekilde düzenleme yapılarak büyük harflerle yazılacak. \n",
    "\n",
    "* islenmis                  I S L E N M I S\n",
    "* olan                      O L A N\n",
    "* alldata                   A L L D A T A\n",
    "* icerisinde                I C E R I S I N D E \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data dosyasının içerisinde yazan veriler satır ve bütün olarak isimlendirildi\n",
    "with open(\"all_data.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# her bir satırda bir cümle olacak şekilde düzenleme yapıldı\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "\n",
    "with open(\"all_data.txt\", \"r+\") as f:\n",
    "    text = text.lower()\n",
    "    f.write(text)\n",
    "\n",
    "with open(\"all_data.txt\", \"r+\", encoding=\"ISO-8859-1\") as f, open(\"tempdata.txt\", \"w\", encoding=\"ISO-8859-1\") as g:\n",
    "\n",
    "    for line in f:\n",
    "        line = sent_tokenize(line)\n",
    "        for sentence in line:\n",
    "            g.write(sentence + \"\\n\")\n",
    "\n",
    "os.replace('tempdata.txt','all_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noktalama şaretleri kaldırıldı\n",
    "import string\n",
    "\n",
    "def remove_punctuation(data_text):\n",
    "    data_text = data_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return data_text\n",
    "\n",
    "# Türkçe harfler değiştirildi ve yazı arasında kalan noktalama işaretleri de replace edildi. \n",
    "def remove_turkish_characters_and_apostrophe(data_text):\n",
    "    d ={\"ç\":\"c\", \"ğ\":\"g\", \"ı\":\"i\", \"ö\":\"o\", \"ş\":\"s\", \"ü\":\"u\", \"'\":\"\",\"\\\"\":\"\", \"`\":\"\", \"’\":\"\", \"‘\":\"\", \"”\":\"\", \"“\":\"\", \"i̇\":\"i\", \"-\":\"\"}\n",
    "    for key, value in d.items():\n",
    "        data_text = data_text.replace(key, value)\n",
    "\n",
    "    data_text = remove_punctuation(data_text)\n",
    "    return data_text\n",
    "    \n",
    "text = remove_turkish_characters_and_apostrophe(text)\n",
    "\n",
    "with open(\"all_data.txt\", \"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamamen sayısal veriden oluşan satırlar silindi\n",
    "import os\n",
    "\n",
    "with open('all_data.txt') as infile, open('tempdata.txt', 'w') as outfile:\n",
    "  for line in infile:\n",
    "    if not any(i.isdigit() for i in line) and line.strip():\n",
    "      outfile.write(line)\n",
    "\n",
    "os.replace('tempdata.txt','all_data.txt')\n",
    "\n",
    "# veriler set'e çevrilerek, duplicate olan satırlar yeni dosyaya eklenmedi.\n",
    "with open('all_data.txt', 'r') as f:\n",
    "    unique_lines = set(f.readlines())\n",
    "with open('all_data.txt', 'w') as f:\n",
    "    f.writelines(unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text kelimelere ayrıldı ve her bir kelime bir satırda olacak şekilde düzenleme yapıldı.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "with open(\"all_data.txt\", \"r+\", encoding=\"ISO-8859-1\") as f, open(\"tempdata.txt\", \"w\", encoding=\"ISO-8859-1\") as g:\n",
    "\n",
    "    for line in f:\n",
    "        line = word_tokenize(line)\n",
    "        for word in line:\n",
    "            g.write(word + \"\\n\")\n",
    "\n",
    "os.replace('tempdata.txt','all_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# her bir satırda yer alan kelimelerden en çok tekrar eden 100 kelime başka bir dosyaya aktarıldı.\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "with open(\"all_data.txt\", \"r\", encoding=\"ISO-8859-1\") as f, open(\"top_hundred_words.txt\", \"w\") as g:\n",
    "    words = f.readlines()\n",
    "\n",
    "    resulting_count = Counter(words)\n",
    "    g.writelines(dict(resulting_count.most_common(100)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en çok tekrar eden 100 kelime içerisindeki en uzun kelime bulundu ve ona göre boşluk ayarlaması yapılarak \n",
    "#karşılarında aynı hizada büyük harfle olacak şekilde yazımı gerçekleştirildi. \n",
    "import os\n",
    "\n",
    "with open(\"top_hundred_words.txt\", \"r+\") as f:\n",
    "    \n",
    "    for lenOfWords in f:\n",
    "        maxlen =len(lenOfWords)\n",
    "        for findmax in f:\n",
    "            if len(findmax) > maxlen:\n",
    "                maxlen = len(findmax)\n",
    "with open(\"top_hundred_words.txt\", \"r\") as f, open(\"tempdata.txt\", \"w\") as g:\n",
    "    for i in f:\n",
    "        gap = (maxlen+10) - len(i)\n",
    "        g.write(i.rstrip() + gap*\" \" + i.upper())\n",
    "\n",
    "os.replace('tempdata.txt','top_hundred_words.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
